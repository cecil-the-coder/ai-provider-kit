# AI Provider Kit - Streaming Demo Client

This demo showcases the **real-time Server-Sent Events (SSE) streaming** capabilities newly implemented across all providers in the AI Provider Kit.

## ğŸ”— Config Compatibility

**This demo uses the same config format as `demo-client`**, so you can share configuration files between them!

## What's New?

All providers now support **real streaming**:
- âœ… **Anthropic** - Real SSE with event-based parsing
- âœ… **Gemini** - Dual API streaming (CloudCode + standard)
- âœ… **Qwen** - OpenAI-compatible streaming
- âœ… **Cerebras** - High-performance streaming
- âœ… **OpenRouter** - Gateway streaming with multiple models
- âœ… **OpenAI** - Reference implementation (existing)

Previously, only OpenAI had real streaming. Now all providers use actual SSE instead of mock streams!

## Features

### 1. **Real-time Streaming Display**
See AI responses character-by-character as they're generated, not after completion.

### 2. **Side-by-Side Comparison**
Compare streaming vs non-streaming mode to see the dramatic UX difference.

### 3. **Performance Metrics**
Track:
- Time to first chunk
- Total chunks received
- Throughput (chars/sec)
- Total duration

### 4. **Multi-Provider Support**
Test any provider or all at once.

## Quick Start

### Option 1: Use your existing demo-client config

```bash
# Use the config from demo-client (supports OAuth, multi-key, etc.)
./demo-streaming -config ../demo-client/config.yaml

# Test specific provider
./demo-streaming -config ../demo-client/config.yaml -provider anthropic

# Compare streaming vs non-streaming
./demo-streaming -config ../demo-client/config.yaml -provider anthropic -compare
```

### Option 2: Use the simplified config

### 1. Set up environment variables

```bash
export ANTHROPIC_API_KEY="your-key"
export GEMINI_API_KEY="your-key"
export QWEN_API_KEY="your-key"
export CEREBRAS_API_KEY="your-key"
export OPENROUTER_API_KEY="your-key"
export OPENAI_API_KEY="your-key"
```

### 2. Run the demo

```bash
# Test enabled providers (from config)
./demo-streaming

# Test single provider with streaming
./demo-streaming -provider anthropic

# Test all enabled providers
./demo-streaming -provider all

# Compare streaming vs non-streaming
./demo-streaming -provider anthropic -compare

# Custom prompt
./demo-streaming -provider gemini -prompt "Explain quantum computing in simple terms"
```

## Usage Examples

### Basic Streaming Test

```bash
go run . -provider anthropic
```

Output:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       AI Provider Kit - Streaming Demo                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Providers to test: [anthropic]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Provider: anthropic
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ Prompt: Write a haiku about artificial intelligence

ğŸŒŠ Streaming response:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silicon minds think                                      â”‚
â”‚ Learning patterns, growing wise                          â”‚
â”‚ Future unfolds now                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š Statistics:
   â±ï¸  Duration: 1.2s
   ğŸ“¦ Chunks received: 15
   ğŸ“ Total characters: 67
   âš¡ Avg chunk size: 4.5 chars
   ğŸš€ Throughput: 55.8 chars/sec
```

### Comparison Mode

```bash
go run . -provider anthropic -compare
```

Shows both modes side-by-side:
- **Non-Streaming**: Wait 3s, then entire response appears
- **Streaming**: First chunk in 0.3s, see output in real-time

### Test All Providers

```bash
go run . -provider all
```

Tests all configured providers sequentially.

## Configuration

### Config Format

This demo supports the **same config format as demo-client**, including:

- âœ… **API Key authentication** (single or multi-key failover)
- âœ… **OAuth authentication** (multi-OAuth with automatic token refresh)
- âœ… **Enabled providers list** (controls which providers to test)
- âœ… **All provider-specific settings** (models, base URLs, project IDs, etc.)

### Example config.yaml

```yaml
providers:
  # List of enabled providers
  enabled:
    - anthropic
    - gemini
    - qwen

  # Provider configurations
  anthropic:
    type: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    default_model: "claude-3-5-sonnet-20241022"

    # Optional: Multi-key failover
    # api_keys:
    #   - "${ANTHROPIC_API_KEY_1}"
    #   - "${ANTHROPIC_API_KEY_2}"

    # Optional: Multi-OAuth
    # oauth_credentials:
    #   - id: "default"
    #     access_token: "..."
    #     refresh_token: "..."
    #     expires_at: "2025-12-31T23:59:59Z"

  gemini:
    type: "gemini"
    api_key: "${GEMINI_API_KEY}"
    default_model: "gemini-2.0-flash-exp"

  # Add more providers...
```

### Using demo-client config

You can directly use your existing demo-client configuration:

```bash
./demo-streaming -config ../demo-client/config.yaml
```

This is useful if you've already set up OAuth credentials or multi-key configurations in demo-client.

## Command-Line Options

| Flag | Default | Description |
|------|---------|-------------|
| `-config` | `config.yaml` | Path to configuration file (can use `../demo-client/config.yaml`) |
| `-provider` | _(uses enabled list)_ | Provider to test (`anthropic`, `gemini`, `qwen`, `cerebras`, `openrouter`, `openai`, or `all`) |
| `-prompt` | _(haiku about AI)_ | Custom prompt to test |
| `-compare` | `false` | Enable side-by-side comparison mode |

**Note**: If `-provider` is not specified, the demo will test all providers in the `enabled` list from your config.

## How Streaming Works

### Architecture

All providers now implement the SSE pattern based on OpenAI's implementation:

```go
type ProviderStream struct {
    reader   *bufio.Reader  // Line-by-line SSE reading
    response *http.Response // HTTP response with stream
    mutex    sync.Mutex     // Thread-safe operations
}

func (s *ProviderStream) Next() (types.ChatCompletionChunk, error) {
    // Read SSE event line-by-line
    line, err := s.reader.ReadString('\n')

    // Parse "data: {...}" format
    // Handle "[DONE]" or provider-specific completion signals
    // Return chunk with content
}
```

### Provider-Specific Details

- **Anthropic**: Uses event types (`content_block_delta`, `message_stop`)
- **Gemini**: Supports both CloudCode and standard API streaming
- **Qwen**: OpenAI-compatible format with delta messages
- **Cerebras**: Standard SSE with `[DONE]` signal
- **OpenRouter**: OpenAI-compatible gateway format

## Benefits of Real Streaming

### User Experience
- âœ… **Immediate feedback** - See results start appearing in <1s
- âœ… **Progress indication** - Know the AI is working
- âœ… **Faster perceived speed** - Feels instant vs waiting 3-5s
- âœ… **Early cancellation** - Stop if output is wrong

### Technical Benefits
- âœ… **Lower memory footprint** - Process chunks incrementally
- âœ… **Better error handling** - Detect issues early
- âœ… **Network efficiency** - Start processing before complete response
- âœ… **Scalability** - Handle longer responses without buffering

## Comparison: Before vs After

### Before (MockStream)
```
Request â†’ Wait 3s â†’ Entire response arrives â†’ Display
```

User sees nothing for 3 seconds, then everything at once.

### After (Real SSE Streaming)
```
Request â†’ 0.3s â†’ First chunk â†’ Progressive chunks â†’ Complete
          â†“       â†“             â†“
          User    Display       Display
          waits   immediately   incrementally
```

User sees output start in 0.3s, then watches it build in real-time.

## Performance Metrics

Typical streaming characteristics:

| Provider | Time to First Chunk | Avg Chunk Size | Throughput |
|----------|---------------------|----------------|------------|
| Anthropic | 200-400ms | 3-5 chars | 50-80 chars/s |
| Gemini | 300-500ms | 4-8 chars | 60-100 chars/s |
| Qwen | 250-450ms | 3-6 chars | 45-75 chars/s |
| Cerebras | 150-300ms | 5-10 chars | 80-120 chars/s |
| OpenRouter | 300-600ms | 4-7 chars | 40-70 chars/s |

*Note: Actual metrics vary by model, prompt, and network conditions.*

## Troubleshooting

### "Provider does not support streaming"
Ensure you're using a recent version of the provider implementation with SSE support.

### No streaming output appears
- Check API key is valid
- Verify network connectivity
- Try with `-compare` to see if non-streaming works

### Slow streaming
- Check network latency
- Try a different model (e.g., Haiku instead of Opus)
- Some providers may throttle free tiers

## Development

To add this demo to your workflow:

```bash
cd examples/demo-client-streaming
go mod tidy
go build
./demo-client-streaming -provider all
```

## Related Documentation

- [FEATURE_ROADMAP.md](../../FEATURE_ROADMAP.md) - Full feature roadmap with streaming checkboxes marked as complete
- [Provider Implementations](../../pkg/providers/) - Source code for each provider's streaming implementation
- [OpenAI Reference](../../pkg/providers/openai/openai.go) - Original SSE implementation pattern

## Credits

Streaming implementation based on OpenAI's SSE pattern, adapted for each provider's specific API format and event structure.
